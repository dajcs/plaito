{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb0b50e-4d1d-4057-a9a9-c9d372cbc027",
   "metadata": {},
   "source": [
    "# Study Summaries Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47892fd5-0e3f-4ad4-b29c-360843656639",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc55b3d-c941-468f-8c51-d41e07f5075c",
   "metadata": {},
   "source": [
    "We are going to organize both summaries into predefined themes.  \n",
    "\n",
    "To analyze both summaries we define 4 themes for both summaries in such a way that the theme content is discussing roughly the same portion of the original study.\n",
    "\n",
    "The generic themes which are easily identifiable at both summaries are `Introduction`, `Methodology`, `Findings` and `Conclusion`.\n",
    "\n",
    "We are going to define a dictionary for both summaries a dictionary with the aforementioned keys and the values will be the actually title in the summary text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b95b0543-f60a-48bf-84c9-190d7bc79f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySummary.txt themes\n",
    "my_themes = {\n",
    "    'Introduction': 'Introduction and Motivation',\n",
    "    'Methodology' : 'Research Methodology',\n",
    "    'Findings'    : 'Findings and Analysis',\n",
    "    'Conclusion'  : 'Limitations and Future Work'}\n",
    "\n",
    "# LLM_Summary.txt themes\n",
    "llm_themes = {\n",
    "    'Introduction': 'Introduction',\n",
    "    'Methodology' : 'Methodology',\n",
    "    'Findings'    : 'Key Findings',\n",
    "    'Conclusion'  : 'Conclusion and Future Work'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0bdef8-3e22-4f37-9a54-60f3cb3f6b9e",
   "metadata": {},
   "source": [
    "We are defining a function that is splitting the summaries text into sections based on the text headings.  \n",
    "The function is going to return a sections dictionary with the generic headings `Introduction`, `Methodology`, `Findings` and `Conclusion` as keys and the correponding text as values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "624dee7c-3ebb-4093-bb05-e5a1c4c795dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_sections(text, themes):\n",
    "    \"\"\"\n",
    "    Splits text into sections based on the provided theme headings.\n",
    "    Returns a dictionary with theme as key and corresponding text as value.\n",
    "    \"\"\"\n",
    "    sections = {}\n",
    "    # get a list of the actual text headings\n",
    "    text_headings = themes.values()\n",
    "    # create a reverse mapping of actual text_headings -> generic headings\n",
    "    generic_headings = {v:k for k,v in themes.items()}\n",
    "    # Create a regex pattern that matches any of the text headings.\n",
    "    # (Assuming that text headings appear at the beginning of a line)\n",
    "    pattern = r'(?m)^(' + '|'.join(re.escape(heading) for heading in text_headings) + r')'\n",
    "    \n",
    "    # Find all matches and split text accordingly.\n",
    "    splits = re.split(pattern, text)\n",
    "    # re.split returns a list where headings are also part of the result.\n",
    "    # The first element is any text before the first heading (if any).\n",
    "    current_heading = None\n",
    "    for segment in splits:\n",
    "        segment = segment.strip()\n",
    "        if segment in text_headings:\n",
    "            current_heading = segment\n",
    "            sections[generic_headings[current_heading]] = \"\"\n",
    "        elif current_heading:\n",
    "            sections[generic_headings[current_heading]] += segment + \"\\n\"\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b741c147-0509-40bb-b048-5aa6eafdff6e",
   "metadata": {},
   "source": [
    "Reading the summary files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "994369f7-ba24-4bcb-a0f8-1d01917c4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files\n",
    "with open(\"MySummary.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    my_summary_text = file.read()\n",
    "\n",
    "with open(\"LLM_Summary.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    llm_summary_text = file.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462aa59-f5b9-4bd2-9a08-78edac1ab8ed",
   "metadata": {},
   "source": [
    "Extracting the sections for each summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5f0c7d6a-e131-4129-a021-57d1be7814d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sections for each summary\n",
    "my_sections = extract_sections(my_summary_text, my_themes)\n",
    "llm_sections = extract_sections(llm_summary_text, llm_themes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9342769-4ee4-486f-bb4d-b01fdf0083fb",
   "metadata": {},
   "source": [
    "Checking and validation the section split result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "31adfe9e-7e47-4d3d-8b7c-351a194655c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_sections length: 4\n",
      "llm_sections length: 4\n"
     ]
    }
   ],
   "source": [
    "# check the number of sections\n",
    "print(\"my_sections length:\", len(my_sections))\n",
    "print(\"llm_sections length:\", len(llm_sections))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f300d140-19c7-4476-a2b9-7eec2adb4e0f",
   "metadata": {},
   "source": [
    "Visually inspecting the last section for both cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5d71b72a-9864-4fe9-a4e8-1b0e35f26686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Introduction', 'Methodology', 'Findings', 'Conclusion'])\n"
     ]
    }
   ],
   "source": [
    "print(my_sections.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d649b2b6-2b74-4096-8e41-8ee0775260dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While domain models can clearly highlight missing requirements, this study did not evaluate whether analysts effectively identify and correct those omissions in practice. Future research should include user studies to explore the practical effectiveness of domain models in supporting requirements validation.\n",
      "\n",
      "Conclusion\n",
      "This empirical study provides concrete evidence supporting domain models' value as effective tools for completeness checking in natural-language requirements specifications. By systematically highlighting omissions, particularly entirely missing requirements, domain models can significantly improve requirements quality, making them valuable components of requirements engineering practice.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(my_sections['Conclusion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e2988d0e-9052-47d0-b2cc-04ebd1d9c7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Introduction', 'Methodology', 'Findings', 'Conclusion'])\n"
     ]
    }
   ],
   "source": [
    "print(llm_sections.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f301d174-d866-405d-8e9f-e2b8636c221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The study provides empirical evidence that domain models can help identify missing and under-specified requirements, though their effectiveness depends on how frequently concepts are referenced in the requirements. The results suggest that domain models should be complemented by other techniques for completeness checking. Future work should focus on user studies to evaluate whether analysts can effectively leverage domain models in practice.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm_sections['Conclusion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717d357e-9b7e-432d-8a74-63363c08905d",
   "metadata": {},
   "source": [
    "## Diffing with Python's `difflib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a57ef7-d23d-4b02-acb2-821ac506dea6",
   "metadata": {},
   "source": [
    "Using the built-in `difflib` module to compute and print the similarity ratio between two sections.  \n",
    "We are defining the function `similarity_ratios` which is going to compute the similarity between the sections of the two summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "877fa2df-3fbf-4f40-b16e-c65b5306727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def compute_similarity_ratios(sequences1, sequences2):\n",
    "\n",
    "    ratios = {}\n",
    "    for theme in sequences1:\n",
    "        ratio = SequenceMatcher(\n",
    "            None,\n",
    "            sequences1[theme],\n",
    "            sequences2[theme]\n",
    "        ).ratio()\n",
    "        ratios[theme] = ratio\n",
    "    return ratios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db323b7-4d52-44d0-a0a9-818a08a23c8b",
   "metadata": {},
   "source": [
    "Computing and displaying the similarity ratios between `my_sections` and `llm_sections`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5102a322-e167-4c7a-b374-69b380cd9898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction: 0.05\n",
      "Methodology : 0.13\n",
      "Findings    : 0.10\n",
      "Conclusion  : 0.05\n"
     ]
    }
   ],
   "source": [
    "my_llm_similarity_ratios = compute_similarity_ratios(my_sections, llm_sections)\n",
    "for theme in my_llm_similarity_ratios:\n",
    "    print(f'{theme:12}: {my_llm_similarity_ratios[theme]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b258417-3e8e-4ee2-aba4-cdaa306a9688",
   "metadata": {},
   "source": [
    "We can observe that the `Introduction` and `Conclusion` sections got only `5%` similarity ratio, while the `Methodology` got `13%` and the `Findings` section got `10%`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fed4aa-3adc-49f4-bccd-24669aef7d86",
   "metadata": {},
   "source": [
    "## TF-IDF and Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b58a27-77ba-4ca6-8382-712cd96660c0",
   "metadata": {},
   "source": [
    "Converting each section into a vector representation using TF-IDF (via scikit-learn), \n",
    "then calculating the cosine similarity.  \n",
    "This method highlights the overall textual differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6237f1de-2ba4-4c0d-b8d4-6d04eed8f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def compute_cosine_similarities(sections1, sections2):\n",
    "    cos_similarities = {}\n",
    "    for theme in sections1:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        texts = [sections1[theme], sections2[theme]]\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        # Use the imported cosine_similarity function from scikit-learn\n",
    "        sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "        cos_similarities[theme] = sim[0][0]\n",
    "    return cos_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "88bd80fc-8937-4974-9b8b-b924a7a2be6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction: 0.65\n",
      "Methodology : 0.59\n",
      "Findings    : 0.65\n",
      "Conclusion  : 0.47\n"
     ]
    }
   ],
   "source": [
    "my_llm_cosine_similarities = compute_cosine_similarities(my_sections, llm_sections)\n",
    "for theme in my_llm_cosine_similarities:\n",
    "    print(f'{theme:12}: {my_llm_cosine_similarities[theme]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276d408-c8c4-424a-8cda-8467cfc42a07",
   "metadata": {},
   "source": [
    "## Embedding-Based Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "daed313f-1e2d-43e7-ad51-69dbbf16e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity: 0.8764227628707886\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "def compute_semantic_similarities(sections1, sections2):\n",
    "    sem_similarities = {}\n",
    "    for theme in sections1:\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        embeddings = model.encode([my_sections[theme], llm_sections[theme]], convert_to_tensor=True)\n",
    "        sem_similarities[theme] = util.pytorch_cos_sim(embeddings[0], embeddings[1])\n",
    "    return sem_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a35e81fe-811d-4d9d-b606-5a9297dd77eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction: 0.86\n",
      "Methodology : 0.88\n",
      "Findings    : 0.92\n",
      "Conclusion  : 0.88\n"
     ]
    }
   ],
   "source": [
    "my_llm_semantic_similarities = compute_semantic_similarities(my_sections, llm_sections)\n",
    "for theme in my_llm_semantic_similarities:\n",
    "    print(f'{theme:12}: {my_llm_semantic_similarities[theme][0][0]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a122d639-15d0-43ff-8cac-96b64d63a1db",
   "metadata": {},
   "source": [
    "## Keyword Extraction Using RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "6177eaa5-5797-430d-90ca-ddcbba07d61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Keywords: {'completeness', 'shall', 'specified requirements', 'statements', 'checking', 'models', 'specification', 'domain models', 'requirements', 'uml class diagrams'} \n",
      "\n",
      "\n",
      "Unique to MySummary: {'external domain knowledge', 'domain model • abstract', 'using domain models', 'every important domain concept', '• models typically include domain concepts', 'feature', 'style', 'specification includes', 'system shall ...\").', 'g .,', 'refers', 'external sources', 'explicitly capture domain concepts', 'ensuring', 'software requirements', 'included within', 'objects ).', 'absence', 'key concepts completeness • internal completeness', '— external completeness involves cross', 'checks', 'real', 'structured representations', 'appear', 'single function', 'implementation', 'thus', 'e', 'system design', 'constraints', 'structured', '• functional requirements', 'conditions', 'avoiding costly oversights', 'underlying hypothesis', 'specification contains', 'paper empirically investigates', 'practical usefulness', 'omission types • unspecified requirements', 'requirements specification', 'relationships', 'gaps', 'undefined internal references', 'completeness checking', 'relevant information', 'generalizations ).', 'external knowledge sources', 'attributes', 'ensuring external completeness', 'requirements (\" shall', 'requirements missing specific details', 'undefined references within', '• external completeness', 'unlike internal completeness —', 'classes ),', 'associations', 'statement specifies', 'domain model element', '•', 'paper addresses external completeness', 'necessary information relative', 'common method proposed', 'world scenarios', 'requirements might indicate missing information', 'entire missing functional requirements', 'relation', 'domain knowledge using uml class diagrams', 'critical'} \n",
      "\n",
      "\n",
      "Unique to LLM_Summary: {'omissions', 'represented', 'categorized', 'internal', 'domain model contains information', 'uml class diagrams useful', 'requirements validation', 'internal completeness ensures', 'self', 'crucial yet challenging aspect', 'necessary system information', 'three industrial domains', 'external completeness', 'detecting omissions', 'given application domain', 'analyzed', 'external completeness ensures', 'domain modeling', 'approach', 'relationships within', 'help identify missing', 'research question', 'authors conduct', 'check natural', 'cross', 'method', 'shall statements ?\"', 'since external completeness cannot', 'domains', 'absolute terms', 'improving', 'case study research', 'missing', 'functional requirements expressed', 'used', 'structured representation', 'captured', 'language', 'randomized simulation process', 'sensitivity', 'contained', 'guaranteed', 'effectiveness', 'sensitivity refers', 'nl', 'requirements completeness', 'study empirically evaluates', 'experts construct domain models', 'domain model', 'empirical study', 'suggested', 'key concepts', 'domain models represented', 'central research question', 'whether', 'underspecified elements'}\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt_tab')\n",
    "\n",
    "def extract_keywords(text):\n",
    "    # Initialize RAKE with NLTK's stopwords\n",
    "    rake = Rake()\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    # Get ranked phrases, higher rank means more important\n",
    "    return set(rake.get_ranked_phrases())\n",
    "\n",
    "# Example sections from each summary\n",
    "my_intro = my_sections['Introduction']\n",
    "llm_intro = llm_sections['Introduction']\n",
    "\n",
    "# Extract keywords\n",
    "my_keywords = extract_keywords(my_intro)\n",
    "llm_keywords = extract_keywords(llm_intro)\n",
    "\n",
    "# Compare the keyword sets\n",
    "common_keywords = my_keywords.intersection(llm_keywords)\n",
    "unique_to_my = my_keywords - llm_keywords\n",
    "unique_to_llm = llm_keywords - my_keywords\n",
    "\n",
    "print(\"Common Keywords:\", common_keywords, '\\n\\n')\n",
    "print(\"Unique to MySummary:\", unique_to_my, '\\n\\n')\n",
    "print(\"Unique to LLM_Summary:\", unique_to_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebca0488-61d3-4a21-9dd2-de881e5f7283",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
