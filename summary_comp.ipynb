{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb0b50e-4d1d-4057-a9a9-c9d372cbc027",
   "metadata": {},
   "source": [
    "# Study Summaries Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47892fd5-0e3f-4ad4-b29c-360843656639",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc55b3d-c941-468f-8c51-d41e07f5075c",
   "metadata": {},
   "source": [
    "We are going to organize both summaries into predefined themes.  \n",
    "\n",
    "To analyze both summaries we define 4 themes for both summaries in such a way that the theme content is discussing roughly the same portion of the original study.\n",
    "\n",
    "The generic themes which are easily identifiable at both summaries are `Introduction`, `Methodology`, `Findings` and `Conclusion`.\n",
    "\n",
    "We are going to define a dictionary for both summaries a dictionary with the aforementioned keys and the values will be the actually title in the summary text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b95b0543-f60a-48bf-84c9-190d7bc79f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySummary.txt themes\n",
    "my_themes = {\n",
    "    'Introduction': 'Introduction and Motivation',\n",
    "    'Methodology' : 'Research Methodology',\n",
    "    'Findings'    : 'Findings and Analysis',\n",
    "    'Conclusion'  : 'Limitations and Future Work'}\n",
    "\n",
    "# LLM_Summary.txt themes\n",
    "llm_themes = {\n",
    "    'Introduction': 'Introduction',\n",
    "    'Methodology' : 'Methodology',\n",
    "    'Findings'    : 'Key Findings',\n",
    "    'Conclusion'  : 'Conclusion and Future Work'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0bdef8-3e22-4f37-9a54-60f3cb3f6b9e",
   "metadata": {},
   "source": [
    "We are defining a function that is splitting the summaries text into sections based on the text headings.  \n",
    "The function is going to return a sections dictionary with the generic headings `Introduction`, `Methodology`, `Findings` and `Conclusion` as keys and the correponding text as values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "624dee7c-3ebb-4093-bb05-e5a1c4c795dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_sections(text, themes):\n",
    "    \"\"\"\n",
    "    Splits text into sections based on the provided theme headings.\n",
    "    Returns a dictionary with theme as key and corresponding text as value.\n",
    "    \"\"\"\n",
    "    sections = {}\n",
    "    # get a list of the actual text headings\n",
    "    text_headings = themes.values()\n",
    "    # create a reverse mapping of actual text_headings -> generic headings\n",
    "    generic_headings = {v:k for k,v in themes.items()}\n",
    "    # Create a regex pattern that matches any of the text headings.\n",
    "    # (Assuming that text headings appear at the beginning of a line)\n",
    "    pattern = r'(?m)^(' + '|'.join(re.escape(heading) for heading in text_headings) + r')'\n",
    "    \n",
    "    # Find all matches and split text accordingly.\n",
    "    splits = re.split(pattern, text)\n",
    "    # re.split returns a list where headings are also part of the result.\n",
    "    # The first element is any text before the first heading (if any).\n",
    "    current_heading = None\n",
    "    for segment in splits:\n",
    "        segment = segment.strip()\n",
    "        if segment in text_headings:\n",
    "            current_heading = segment\n",
    "            sections[generic_headings[current_heading]] = \"\"\n",
    "        elif current_heading:\n",
    "            sections[generic_headings[current_heading]] += segment + \"\\n\"\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b741c147-0509-40bb-b048-5aa6eafdff6e",
   "metadata": {},
   "source": [
    "Reading the summary files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "994369f7-ba24-4bcb-a0f8-1d01917c4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files\n",
    "with open(\"MySummary.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    my_summary = file.read()\n",
    "\n",
    "with open(\"LLM_Summary.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    llm_summary = file.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462aa59-f5b9-4bd2-9a08-78edac1ab8ed",
   "metadata": {},
   "source": [
    "Extracting the sections for each summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c7d6a-e131-4129-a021-57d1be7814d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sections for each summary\n",
    "my_sections = extract_sections(my_summary, my_themes)\n",
    "llm_sections = extract_sections(llm_summary, llm_themes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9342769-4ee4-486f-bb4d-b01fdf0083fb",
   "metadata": {},
   "source": [
    "Checking and validation the section split result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "31adfe9e-7e47-4d3d-8b7c-351a194655c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_sections length: 4\n",
      "llm_sections length: 4\n"
     ]
    }
   ],
   "source": [
    "# check the number of sections\n",
    "print(\"my_sections length:\", len(my_sections))\n",
    "print(\"llm_sections length:\", len(llm_sections))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f300d140-19c7-4476-a2b9-7eec2adb4e0f",
   "metadata": {},
   "source": [
    "Visually inspecting the last section for both cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5d71b72a-9864-4fe9-a4e8-1b0e35f26686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Introduction', 'Methodology', 'Findings', 'Conclusion'])\n"
     ]
    }
   ],
   "source": [
    "print(my_sections.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d649b2b6-2b74-4096-8e41-8ee0775260dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While domain models can clearly highlight missing requirements, this study did not evaluate whether analysts effectively identify and correct those omissions in practice. Future research should include user studies to explore the practical effectiveness of domain models in supporting requirements validation.\n",
      "\n",
      "Conclusion\n",
      "This empirical study provides concrete evidence supporting domain models' value as effective tools for completeness checking in natural-language requirements specifications. By systematically highlighting omissions, particularly entirely missing requirements, domain models can significantly improve requirements quality, making them valuable components of requirements engineering practice.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(my_sections['Conclusion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e2988d0e-9052-47d0-b2cc-04ebd1d9c7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Introduction', 'Methodology', 'Findings', 'Conclusion'])\n"
     ]
    }
   ],
   "source": [
    "print(llm_sections.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f301d174-d866-405d-8e9f-e2b8636c221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The study provides empirical evidence that domain models can help identify missing and under-specified requirements, though their effectiveness depends on how frequently concepts are referenced in the requirements. The results suggest that domain models should be complemented by other techniques for completeness checking. Future work should focus on user studies to evaluate whether analysts can effectively leverage domain models in practice.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm_sections['Conclusion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717d357e-9b7e-432d-8a74-63363c08905d",
   "metadata": {},
   "source": [
    "## Diffing with Python's `difflib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a57ef7-d23d-4b02-acb2-821ac506dea6",
   "metadata": {},
   "source": [
    "Using the built-in `difflib` module to compute and print the similarity ratio between two sections.  \n",
    "We are defining the function `similarity_ratios` which is going to compute the similarity between the sections of the two summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "877fa2df-3fbf-4f40-b16e-c65b5306727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def compute_similarity_ratios(sequences1, sequences2):\n",
    "\n",
    "    ratios = {}\n",
    "    for theme in sequences1:\n",
    "        ratio = SequenceMatcher(\n",
    "            None,\n",
    "            sequences1[theme],\n",
    "            sequences2[theme]\n",
    "        ).ratio()\n",
    "        ratios[theme] = ratio\n",
    "    return ratios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db323b7-4d52-44d0-a0a9-818a08a23c8b",
   "metadata": {},
   "source": [
    "Computing and displaying the similarity ratios between `my_sections` and `llm_sections`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5102a322-e167-4c7a-b374-69b380cd9898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction: 0.05\n",
      "Methodology : 0.13\n",
      "Findings    : 0.10\n",
      "Conclusion  : 0.05\n"
     ]
    }
   ],
   "source": [
    "my_llm_similarity_ratios = compute_similarity_ratios(my_sections, llm_sections)\n",
    "for theme in my_llm_similarity_ratios:\n",
    "    print(f'{theme:12}: {my_llm_similarity_ratios[theme]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b258417-3e8e-4ee2-aba4-cdaa306a9688",
   "metadata": {},
   "source": [
    "We can observe that the `Introduction` and `Conclusion` sections got only `5%` similarity ratio, while the `Methodology` got `13%` and the `Findings` section got `10%`.\n",
    "\n",
    "We're getting these low ratios because `difflib` is used mostly for detailed, line-by-line comparison (e.g., program code comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fed4aa-3adc-49f4-bccd-24669aef7d86",
   "metadata": {},
   "source": [
    "## TF-IDF and Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b58a27-77ba-4ca6-8382-712cd96660c0",
   "metadata": {},
   "source": [
    "Converting each section into a vector representation using TF-IDF (via scikit-learn), \n",
    "then calculating the cosine similarity.  \n",
    "This method highlights the overall textual differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6237f1de-2ba4-4c0d-b8d4-6d04eed8f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def compute_cosine_similarities(sections1, sections2):\n",
    "    cos_similarities = {}\n",
    "    for theme in sections1:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        texts = [sections1[theme], sections2[theme]]\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        # Use the imported cosine_similarity function from scikit-learn\n",
    "        sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "        cos_similarities[theme] = sim[0][0]\n",
    "    return cos_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "88bd80fc-8937-4974-9b8b-b924a7a2be6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction: 0.65\n",
      "Methodology : 0.59\n",
      "Findings    : 0.65\n",
      "Conclusion  : 0.47\n"
     ]
    }
   ],
   "source": [
    "my_llm_cosine_similarities = compute_cosine_similarities(my_sections, llm_sections)\n",
    "for theme in my_llm_cosine_similarities:\n",
    "    print(f'{theme:12}: {my_llm_cosine_similarities[theme]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d3f472-9492-409d-9818-ff94d4ab9181",
   "metadata": {},
   "source": [
    "TF-IDF/Cosine Similarity results show a more pronunciated sense of overall content \n",
    "similarity between the two summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276d408-c8c4-424a-8cda-8467cfc42a07",
   "metadata": {},
   "source": [
    "## Embedding-Based Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "daed313f-1e2d-43e7-ad51-69dbbf16e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity: 0.8764227628707886\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "def compute_semantic_similarities(sections1, sections2):\n",
    "    sem_similarities = {}\n",
    "    for theme in sections1:\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        embeddings = model.encode([my_sections[theme], llm_sections[theme]], convert_to_tensor=True)\n",
    "        sem_similarities[theme] = util.pytorch_cos_sim(embeddings[0], embeddings[1])\n",
    "    return sem_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a35e81fe-811d-4d9d-b606-5a9297dd77eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction: 0.86\n",
      "Methodology : 0.88\n",
      "Findings    : 0.92\n",
      "Conclusion  : 0.88\n"
     ]
    }
   ],
   "source": [
    "my_llm_semantic_similarities = compute_semantic_similarities(my_sections, llm_sections)\n",
    "for theme in my_llm_semantic_similarities:\n",
    "    print(f'{theme:12}: {my_llm_semantic_similarities[theme][0][0]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f294e89c-8175-4aef-82b7-8c6b54980ff0",
   "metadata": {},
   "source": [
    "The Embedding -based approach show that there are relatively small semantic differences between the summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a122d639-15d0-43ff-8cac-96b64d63a1db",
   "metadata": {},
   "source": [
    "## Keyword Extraction Using RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "6d18cfe0-dd12-4233-9c7b-e4f99f443497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt_tab')\n",
    "\n",
    "def extract_keywords(text):\n",
    "    # Initialize RAKE (make sure you've downloaded the NLTK stopwords if not already done)\n",
    "    rake = Rake()\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    return set(rake.get_ranked_phrases())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ef96f1-1c5c-4407-a857-9252c7faa63f",
   "metadata": {},
   "source": [
    "Extracting the keywords for the generic themes and feeding the number of keywords into\n",
    "a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "9126bf05-e873-471e-a7c5-44d7714cb7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Introduction Methodology Findings Conclusion\n",
      "Common Keywords                        10           6        7          3\n",
      "Unique to MySummary                    68          29       82         22\n",
      "Unique to LLM_Summary                  56          42       48         16\n",
      "MySummary Common Ratio [%]           12 %        17 %      7 %       12 %\n",
      "LLM_Summary Common Ratio [%]         15 %        12 %     12 %       15 %\n"
     ]
    }
   ],
   "source": [
    "# Defining themes that match the keys in my_sections and llm_sections dictionaries\n",
    "themes = ['Introduction', 'Methodology', 'Findings', 'Conclusion']\n",
    "\n",
    "# Prepare a dictionary to hold the counts for each theme\n",
    "data = {}\n",
    "\n",
    "for theme in themes:\n",
    "    # Get text for the theme; if a theme isn't found, default to an empty string.\n",
    "    my_text = my_sections.get(theme, \"\")\n",
    "    llm_text = llm_sections.get(theme, \"\")\n",
    "    \n",
    "    # Extract keywords from both texts\n",
    "    my_keywords = extract_keywords(my_text)\n",
    "    llm_keywords = extract_keywords(llm_text)\n",
    "    \n",
    "    # Calculate common and unique keyword sets\n",
    "    common_keywords = my_keywords.intersection(llm_keywords)\n",
    "    unique_to_my = my_keywords - llm_keywords\n",
    "    unique_to_llm = llm_keywords - my_keywords\n",
    "    \n",
    "    # Calculate total keywords for each summary\n",
    "    total_my = len(common_keywords) + len(unique_to_my)\n",
    "    total_llm = len(common_keywords) + len(unique_to_llm)\n",
    "    \n",
    "    # Calculate ratios and format as an integer followed by \"%\"\n",
    "    ratio_my = int((len(common_keywords) / total_my * 100)) if total_my > 0 else 0\n",
    "    ratio_llm = int((len(common_keywords) / total_llm * 100)) if total_llm > 0 else 0\n",
    "    \n",
    "    # Store the counts and formatted ratios in a sub-dictionary for this theme\n",
    "    data[theme] = {\n",
    "        'Common Keywords': len(common_keywords),\n",
    "        'Unique to MySummary': len(unique_to_my),\n",
    "        'Unique to LLM_Summary': len(unique_to_llm),\n",
    "        'MySummary Common Ratio [%]': f\"{ratio_my}%\",\n",
    "        'LLM_Summary Common Ratio [%]': f\"{ratio_llm}%\"\n",
    "    }\n",
    "\n",
    "# Create a DataFrame where columns are themes and rows are the categories\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optionally, reorder the rows\n",
    "desired_order = [\n",
    "    'Common Keywords', \n",
    "    'Unique to MySummary', \n",
    "    'Unique to LLM_Summary', \n",
    "    'MySummary Common Ratio [%]', \n",
    "    'LLM_Summary Common Ratio [%]'\n",
    "]\n",
    "df = df.reindex(desired_order)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebca0488-61d3-4a21-9dd2-de881e5f7283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f526cb-6c70-445c-9819-b439d4079329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4bbe39-b8ae-45d8-81d1-bcfe0c0074ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7040de79-1918-4bf5-96f3-7c973209c88c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40aab8d-a228-444c-b8dd-532174efd3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "849a832a-2aac-45b0-b12e-a45bc690a2e9",
   "metadata": {},
   "source": [
    "# Optional Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8058c7ae-c2e5-492b-9401-d778d84830e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0531825-e0ba-43b9-ba42-274cf7c6d894",
   "metadata": {},
   "source": [
    "## Reading the Original Study into a Markdown File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e191dfe5-8929-4825-b6a6-7357e6c967fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..........................................................................] 3582534 / 3582534"
     ]
    }
   ],
   "source": [
    "# downloading the original study pdf\n",
    "import wget\n",
    "url = 'https://link.springer.com/content/pdf/10.1007/s10664-019-09693-x.pdf'\n",
    "pdf_filename = wget.download(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d153bab-6b8d-4f34-9fa5-a33c87d792ac",
   "metadata": {},
   "source": [
    "We are going to convert the original pdf file into markdown with Mistral AI.  \n",
    "We need to register and we can subscribe for a free plan on [Mistral](https://console.mistral.ai/home) homepage and we need to generate and API key.\n",
    "\n",
    "The API key can be stored in the current directory in a file called `.env` in format:\n",
    "```\n",
    "MISTRAL_API_KEY=Ta7...\n",
    "```\n",
    "We should add this file to the `.gitignore` in order to not upload in a public repository. This file can store more API keys in separate lines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ed4112b3-6c3c-470a-a348-b200aeeaf8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Mistral client with API key\n",
    "from mistralai import Mistral\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# api_key = \"your API key\" # plain text API key can be used when notebook is not shared\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ad9fb4b5-a86e-4957-a073-603220ad68b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pathlib import Path\n",
    "from mistralai import DocumentURLChunk, ImageURLChunk, TextChunk\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "89e69533-9f7b-4a0f-a87b-277d86c6bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify PDF file exists\n",
    "pdf_file = Path(pdf_filename)\n",
    "assert pdf_file.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7134a84f-3916-4861-b226-b190344fad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload PDF file to Mistral's OCR service\n",
    "uploaded_file = client.files.upload(\n",
    "    file={\n",
    "        \"file_name\": pdf_file.stem,\n",
    "        \"content\": pdf_file.read_bytes(),\n",
    "    },\n",
    "    purpose=\"ocr\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "9225f3ac-482e-4761-a5d1-64d658fe0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get URL for the uploaded file\n",
    "signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=3)\n",
    "\n",
    "# Process PDF with OCR\n",
    "pdf_response = client.ocr.process(\n",
    "    document=DocumentURLChunk(document_url=signed_url.url),\n",
    "    model=\"mistral-ocr-latest\",\n",
    "    include_image_base64=True\n",
    ")\n",
    "\n",
    "# Convert response to JSON format\n",
    "response_dict = json.loads(pdf_response.model_dump_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "67980cf0-388a-4bef-93a9-52e8a2c0640e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pages', 'model', 'usage_info'])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the json object structure\n",
    "response_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "95e1fe7a-508d-417f-baa4-f1047748c523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the length of the scanned pdf\n",
    "len(response_dict['pages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "d3a4941a-b1c5-43ba-8b9d-5bb22e41acc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "applied. Since the loop of L. 5 has a random component, it has to be run multiple times to account for random variation (L. 4). The scatter plot resulting from running the algorithm of Fig. 5 is the basis for answering RQ1 in Section 4.\n",
       "\n",
       "# 4 Results and Discussion \n",
       "\n",
       "In this section, we (i) discuss the results of our case studies, (ii) answer the research questions posed in Section 3.1, and (iii) argue about the representativeness of the requirements samples used in our case studies.\n",
       "\n",
       "Table 1 provides key statistics about the outcomes of our data collection as per the procedures discussed in Sections 3.3.1, 3.3.2 and 3.3.3. For each case study, the table provides the following information: (1) the number of omittable elements of different types: as explained in Section 3.3.2, an omittable element can be an entire requirement or a certain segment of a requirement, namely a condition, constraint, or object; (2) the number and type of interdependencies between the omittable constraints and objects (the types were discussed in Section 3.3.2); (3) interrater agreement, computed as Cohen's $\\kappa$ (1960), for the identification and classification of omittable segments by two coders. The $\\kappa$ scores indicate strong, almost perfect or perfect agreement in all case studies; (4) the number of domain model elements (of which the number of elements tacit in the requirements is shown in brackets); and (5) the number of trace links from the requirements to the domain model.\n",
       "\n",
       "## Algorithm Simulation\n",
       "\n",
       "Input: - A domain model\n",
       "\n",
       "- Set of requirements, their omittable segments, and the interdependencies between these segments\n",
       "- Set $\\mathcal{L}$ of trace links from the requirements to the domain model\n",
       "- Omission type $T \\in\\{\\mathrm{Req}$, Cond, Cons, Obj $\\}$ to simulate\n",
       "- Number $n$ of simulation runs\n",
       "\n",
       "Output: - Scatter plot Plot showing the percentage of unsupported domain model elements versus the number of omissions\n",
       "\n",
       "1. Let $\\mathcal{K}$ be the set of domain model elements not supported by $\\mathcal{L}$\n",
       "2. Let $\\mathcal{O}$ be the set of all omittable segments of type $T$\n",
       "3. Let Plot be initially empty\n",
       "4. for $i=1$ to $n$ :\n",
       "5. for $j=1$ to $|\\mathcal{O}|$ :\n",
       "6. Randomly pick $j$ elements, $\\mathcal{E}=\\left\\{e_{1}, \\cdots, e_{j}\\right\\}$, from $\\mathcal{O}$\n",
       "7. if $(T=$ Cons or $T=$ Obj $)$\n",
       "\n",
       "8 . Minimally modify $\\mathcal{E}$ to resolve any interdependency violations\n",
       "9. Let $\\mathcal{L}^{-}$be the set of all trace links originating from $\\mathcal{E}$\n",
       "10. $\\mathcal{L}^{\\prime}=\\mathcal{L} \\backslash \\mathcal{L}^{-}$\n",
       "11. Let $\\mathcal{U}$ be the set of domain model elements supported by $\\mathcal{L}$ but not $\\mathcal{L}^{\\prime}$\n",
       "12. Let $\\mathcal{X}$ be the domain elements in $\\mathcal{K}$ that will become unreachable from non- $\\mathcal{K}$ elements if $\\mathcal{U}$ is removed from the domain model\n",
       "13. Place the following datapoint on Plot: $(|\\mathcal{E}|,(|\\mathcal{U}|+|\\mathcal{X}|) / D)$, where $D$ is the total number of domain model elements\n",
       "14. return Plot\n",
       "\n",
       "Fig. 5 Simulation algorithm for computing sensitivity"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displaying a random page from the scanned pdf\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response_dict['pages'][17]['markdown']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "2af55892-f9ba-457e-a8d6-7bf50735be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the pages into a single Markdown file\n",
    "original_study = ''\n",
    "for page in response_dict['pages']:\n",
    "    original_study += page['markdown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "2213dbb6-0d69-42d1-9766-9929e8a06d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup of the original_study markdown file\n",
    "with open(\"original_study.md\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(original_study)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb766b-ebcc-4ef5-86b9-042c0c873f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a17f42f9-bcb2-4931-9f37-6fd334e454b0",
   "metadata": {},
   "source": [
    "## Summarization Quality Evaluation with ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e0d1e-7b38-4061-9757-41581ea5ad24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "35e11838-b2de-40c0-9cfa-949fe3246b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Summary vs Original Study:\n",
      "[{'rouge-1': {'r': 0.08398133748055987, 'p': 0.6506024096385542, 'f': 0.14876032855341545}, 'rouge-2': {'r': 0.01447051085288314, 'p': 0.19020172910662825, 'f': 0.02689486421162684}, 'rouge-l': {'r': 0.07900466562986003, 'p': 0.6120481927710844, 'f': 0.1399449015561703}}]\n",
      "LLM Summary vs Original Study:\n",
      "[{'rouge-1': {'r': 0.0656298600311042, 'p': 0.694078947368421, 'f': 0.11992043036238709}, 'rouge-2': {'r': 0.019403639552729664, 'p': 0.34368932038834954, 'f': 0.036733422252820475}, 'rouge-l': {'r': 0.06314152410575427, 'p': 0.6677631578947368, 'f': 0.1153736841276613}}]\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "# For the 'rouge' package, the order is candidate summary first, then reference.\n",
    "scores_my = rouge.get_scores(my_summary, original_study)\n",
    "scores_llm = rouge.get_scores(llm_summary, original_study)\n",
    "\n",
    "print(\"My Summary vs Original Study:\")\n",
    "print(scores_my)\n",
    "\n",
    "print(\"LLM Summary vs Original Study:\")\n",
    "print(scores_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "6b3590e1-4ba5-4177-a96f-e873cdfc237c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounded My Summary vs Original Study:\n",
      "[{'rouge-1': {'r': 0.08, 'p': 0.65, 'f': 0.15}, 'rouge-2': {'r': 0.01, 'p': 0.19, 'f': 0.03}, 'rouge-l': {'r': 0.08, 'p': 0.61, 'f': 0.14}}]\n",
      "Rounded LLM Summary vs Original Study:\n",
      "[{'rouge-1': {'r': 0.07, 'p': 0.69, 'f': 0.12}, 'rouge-2': {'r': 0.02, 'p': 0.34, 'f': 0.04}, 'rouge-l': {'r': 0.06, 'p': 0.67, 'f': 0.12}}]\n"
     ]
    }
   ],
   "source": [
    "def round_scores(score_list, decimals=2):\n",
    "    rounded_list = []\n",
    "    for score in score_list:\n",
    "        rounded_score = {}\n",
    "        for metric, values in score.items():\n",
    "            rounded_score[metric] = {k: round(v, decimals) for k, v in values.items()}\n",
    "        rounded_list.append(rounded_score)\n",
    "    return rounded_list\n",
    "\n",
    "# Assuming scores_my and scores_llm hold the results you posted:\n",
    "rounded_my = round_scores(scores_my)\n",
    "rounded_llm = round_scores(scores_llm)\n",
    "\n",
    "print(\"Rounded My Summary vs Original Study:\")\n",
    "print(rounded_my)\n",
    "print(\"Rounded LLM Summary vs Original Study:\")\n",
    "print(rounded_llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dccd99-06d0-4595-9c5a-f56988729f0a",
   "metadata": {},
   "source": [
    "Feeding the results into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "9a107842-52c8-4965-874e-c239c9cec1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Metric  My_Recall  My_Precision  My_F1  LLM_Recall  LLM_Precision  LLM_F1\n",
      "0  rouge-1       0.08          0.65   0.15        0.07           0.69    0.12\n",
      "1  rouge-2       0.01          0.19   0.03        0.02           0.34    0.04\n",
      "2  rouge-l       0.08          0.61   0.14        0.06           0.67    0.12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build a DataFrame with rounded ROUGE scores for each metric and summary\n",
    "data = {\n",
    "    'Metric': ['rouge-1', 'rouge-2', 'rouge-l'],\n",
    "    'My_Recall': [round(scores_my[0]['rouge-1']['r'], 2), round(scores_my[0]['rouge-2']['r'], 2), round(scores_my[0]['rouge-l']['r'], 2)],\n",
    "    'My_Precision': [round(scores_my[0]['rouge-1']['p'], 2), round(scores_my[0]['rouge-2']['p'], 2), round(scores_my[0]['rouge-l']['p'], 2)],\n",
    "    'My_F1': [round(scores_my[0]['rouge-1']['f'], 2), round(scores_my[0]['rouge-2']['f'], 2), round(scores_my[0]['rouge-l']['f'], 2)],\n",
    "    'LLM_Recall': [round(scores_llm[0]['rouge-1']['r'], 2), round(scores_llm[0]['rouge-2']['r'], 2), round(scores_llm[0]['rouge-l']['r'], 2)],\n",
    "    'LLM_Precision': [round(scores_llm[0]['rouge-1']['p'], 2), round(scores_llm[0]['rouge-2']['p'], 2), round(scores_llm[0]['rouge-l']['p'], 2)],\n",
    "    'LLM_F1': [round(scores_llm[0]['rouge-1']['f'], 2), round(scores_llm[0]['rouge-2']['f'], 2), round(scores_llm[0]['rouge-l']['f'], 2)]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06dbfe0-258d-4174-9a94-9fe8a230df63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
