{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb0b50e-4d1d-4057-a9a9-c9d372cbc027",
   "metadata": {},
   "source": [
    "# Study Summaries Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47892fd5-0e3f-4ad4-b29c-360843656639",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc55b3d-c941-468f-8c51-d41e07f5075c",
   "metadata": {},
   "source": [
    "We are going to organize both summaries into predefined themes.  \n",
    "\n",
    "To analyze both summaries we define 4 themes for both summaries in such a way that the theme content is discussing roughly the same portion of the original study.\n",
    "\n",
    "The generic themes which are easily identifiable at both summaries are `Introduction`, `Methodology`, `Findings` and `Conclusion`.\n",
    "\n",
    "We are going to define a dictionary for both summaries a dictionary with the aforementioned keys and the values will be the actually title in the summary text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b95b0543-f60a-48bf-84c9-190d7bc79f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySummary.txt themes\n",
    "my_themes = {\n",
    "    'Introduction': 'Introduction and Motivation',\n",
    "    'Methodology' : 'Research Methodology',\n",
    "    'Findings'    : 'Findings and Analysis',\n",
    "    'Conclusion'  : 'Limitations and Future Work'}\n",
    "\n",
    "# LLM_Summary.txt themes\n",
    "llm_themes = {\n",
    "    'Introduction': 'Introduction',\n",
    "    'Methodology' : 'Methodology',\n",
    "    'Findings'    : 'Key Findings',\n",
    "    'Conclusion'  : 'Conclusion and Future Work'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0bdef8-3e22-4f37-9a54-60f3cb3f6b9e",
   "metadata": {},
   "source": [
    "We are defining a function that is splitting the summaries text into sections based on the text headings.  \n",
    "The function is going to return a sections dictionary with the generic headings `Introduction`, `Methodology`, `Findings` and `Conclusion` as keys and the correponding text as values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "624dee7c-3ebb-4093-bb05-e5a1c4c795dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_sections(text, themes):\n",
    "    \"\"\"\n",
    "    Splits text into sections based on the provided theme headings.\n",
    "    Returns a dictionary with theme as key and corresponding text as value.\n",
    "    \"\"\"\n",
    "    sections = {}\n",
    "    # get a list of the actual text headings\n",
    "    text_headings = themes.values()\n",
    "    # create a reverse mapping of actual text_headings -> generic headings\n",
    "    generic_headings = {v:k for k,v in themes.items()}\n",
    "    # Create a regex pattern that matches any of the text headings.\n",
    "    # (Assuming that text headings appear at the beginning of a line)\n",
    "    pattern = r'(?m)^(' + '|'.join(re.escape(heading) for heading in text_headings) + r')'\n",
    "    \n",
    "    # Find all matches and split text accordingly.\n",
    "    splits = re.split(pattern, text)\n",
    "    # re.split returns a list where headings are also part of the result.\n",
    "    # The first element is any text before the first heading (if any).\n",
    "    current_heading = None\n",
    "    for segment in splits:\n",
    "        segment = segment.strip()\n",
    "        if segment in text_headings:\n",
    "            current_heading = segment\n",
    "            sections[generic_headings[current_heading]] = \"\"\n",
    "        elif current_heading:\n",
    "            sections[generic_headings[current_heading]] += segment + \"\\n\"\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b741c147-0509-40bb-b048-5aa6eafdff6e",
   "metadata": {},
   "source": [
    "Reading the summary files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "994369f7-ba24-4bcb-a0f8-1d01917c4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files\n",
    "with open(\"MySummary.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    my_summary = file.read()\n",
    "\n",
    "with open(\"LLM_Summary.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    llm_summary = file.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462aa59-f5b9-4bd2-9a08-78edac1ab8ed",
   "metadata": {},
   "source": [
    "Extracting the sections for each summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c7d6a-e131-4129-a021-57d1be7814d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sections for each summary\n",
    "my_sections = extract_sections(my_summary, my_themes)\n",
    "llm_sections = extract_sections(llm_summary, llm_themes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9342769-4ee4-486f-bb4d-b01fdf0083fb",
   "metadata": {},
   "source": [
    "Checking and validation the section split result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "31adfe9e-7e47-4d3d-8b7c-351a194655c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_sections length: 4\n",
      "llm_sections length: 4\n"
     ]
    }
   ],
   "source": [
    "# check the number of sections\n",
    "print(\"my_sections length:\", len(my_sections))\n",
    "print(\"llm_sections length:\", len(llm_sections))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f300d140-19c7-4476-a2b9-7eec2adb4e0f",
   "metadata": {},
   "source": [
    "Visually inspecting the last section for both cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5d71b72a-9864-4fe9-a4e8-1b0e35f26686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Introduction', 'Methodology', 'Findings', 'Conclusion'])\n"
     ]
    }
   ],
   "source": [
    "print(my_sections.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d649b2b6-2b74-4096-8e41-8ee0775260dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While domain models can clearly highlight missing requirements, this study did not evaluate whether analysts effectively identify and correct those omissions in practice. Future research should include user studies to explore the practical effectiveness of domain models in supporting requirements validation.\n",
      "\n",
      "Conclusion\n",
      "This empirical study provides concrete evidence supporting domain models' value as effective tools for completeness checking in natural-language requirements specifications. By systematically highlighting omissions, particularly entirely missing requirements, domain models can significantly improve requirements quality, making them valuable components of requirements engineering practice.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(my_sections['Conclusion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e2988d0e-9052-47d0-b2cc-04ebd1d9c7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Introduction', 'Methodology', 'Findings', 'Conclusion'])\n"
     ]
    }
   ],
   "source": [
    "print(llm_sections.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f301d174-d866-405d-8e9f-e2b8636c221e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The study provides empirical evidence that domain models can help identify missing and under-specified requirements, though their effectiveness depends on how frequently concepts are referenced in the requirements. The results suggest that domain models should be complemented by other techniques for completeness checking. Future work should focus on user studies to evaluate whether analysts can effectively leverage domain models in practice.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm_sections['Conclusion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717d357e-9b7e-432d-8a74-63363c08905d",
   "metadata": {},
   "source": [
    "## Diffing with Python's `difflib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a57ef7-d23d-4b02-acb2-821ac506dea6",
   "metadata": {},
   "source": [
    "Using the built-in `difflib` module to compute and print the similarity ratio between two sections.  \n",
    "We are defining the function `similarity_ratios` which is going to compute the similarity between the sections of the two summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "877fa2df-3fbf-4f40-b16e-c65b5306727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def compute_similarity_ratios(sequences1, sequences2):\n",
    "\n",
    "    ratios = {}\n",
    "    for theme in sequences1:\n",
    "        ratio = SequenceMatcher(\n",
    "            None,\n",
    "            sequences1[theme],\n",
    "            sequences2[theme]\n",
    "        ).ratio()\n",
    "        ratios[theme] = ratio\n",
    "    return ratios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db323b7-4d52-44d0-a0a9-818a08a23c8b",
   "metadata": {},
   "source": [
    "Computing and displaying the similarity ratios between `my_sections` and `llm_sections`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5102a322-e167-4c7a-b374-69b380cd9898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction: 0.05\n",
      "Methodology : 0.13\n",
      "Findings    : 0.10\n",
      "Conclusion  : 0.05\n"
     ]
    }
   ],
   "source": [
    "my_llm_similarity_ratios = compute_similarity_ratios(my_sections, llm_sections)\n",
    "for theme in my_llm_similarity_ratios:\n",
    "    print(f'{theme:12}: {my_llm_similarity_ratios[theme]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b258417-3e8e-4ee2-aba4-cdaa306a9688",
   "metadata": {},
   "source": [
    "We can observe that the `Introduction` and `Conclusion` sections got only `5%` similarity ratio, while the `Methodology` got `13%` and the `Findings` section got `10%`.\n",
    "\n",
    "We're getting these low ratios because `difflib` is used mostly for detailed, line-by-line comparison (e.g., program code comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fed4aa-3adc-49f4-bccd-24669aef7d86",
   "metadata": {},
   "source": [
    "## TF-IDF and Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b58a27-77ba-4ca6-8382-712cd96660c0",
   "metadata": {},
   "source": [
    "Converting each section into a vector representation using TF-IDF (via scikit-learn), \n",
    "then calculating the cosine similarity.  \n",
    "This method highlights the overall textual differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6237f1de-2ba4-4c0d-b8d4-6d04eed8f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def compute_cosine_similarities(sections1, sections2):\n",
    "    cos_similarities = {}\n",
    "    for theme in sections1:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        texts = [sections1[theme], sections2[theme]]\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        # Use the imported cosine_similarity function from scikit-learn\n",
    "        sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "        cos_similarities[theme] = sim[0][0]\n",
    "    return cos_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "88bd80fc-8937-4974-9b8b-b924a7a2be6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction: 0.65\n",
      "Methodology : 0.59\n",
      "Findings    : 0.65\n",
      "Conclusion  : 0.47\n"
     ]
    }
   ],
   "source": [
    "my_llm_cosine_similarities = compute_cosine_similarities(my_sections, llm_sections)\n",
    "for theme in my_llm_cosine_similarities:\n",
    "    print(f'{theme:12}: {my_llm_cosine_similarities[theme]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d3f472-9492-409d-9818-ff94d4ab9181",
   "metadata": {},
   "source": [
    "TF-IDF/Cosine Similarity results show a more pronunciated sense of overall content \n",
    "similarity between the two summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276d408-c8c4-424a-8cda-8467cfc42a07",
   "metadata": {},
   "source": [
    "## Embedding-Based Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "daed313f-1e2d-43e7-ad51-69dbbf16e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity: 0.8764227628707886\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "def compute_semantic_similarities(sections1, sections2):\n",
    "    sem_similarities = {}\n",
    "    for theme in sections1:\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        embeddings = model.encode([my_sections[theme], llm_sections[theme]], convert_to_tensor=True)\n",
    "        sem_similarities[theme] = util.pytorch_cos_sim(embeddings[0], embeddings[1])\n",
    "    return sem_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a35e81fe-811d-4d9d-b606-5a9297dd77eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction: 0.86\n",
      "Methodology : 0.88\n",
      "Findings    : 0.92\n",
      "Conclusion  : 0.88\n"
     ]
    }
   ],
   "source": [
    "my_llm_semantic_similarities = compute_semantic_similarities(my_sections, llm_sections)\n",
    "for theme in my_llm_semantic_similarities:\n",
    "    print(f'{theme:12}: {my_llm_semantic_similarities[theme][0][0]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f294e89c-8175-4aef-82b7-8c6b54980ff0",
   "metadata": {},
   "source": [
    "The Embedding -based approach show that there are relatively small semantic differences between the summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a122d639-15d0-43ff-8cac-96b64d63a1db",
   "metadata": {},
   "source": [
    "## Keyword Extraction Using RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "6d18cfe0-dd12-4233-9c7b-e4f99f443497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "# Make sure to download the NLTK stopwords and punkt_tab if not already done\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt_tab')\n",
    "\n",
    "def extract_keywords(text):\n",
    "    # Initialize RAKE \n",
    "    rake = Rake()\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    return set(rake.get_ranked_phrases())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ef96f1-1c5c-4407-a857-9252c7faa63f",
   "metadata": {},
   "source": [
    "Extracting the keywords for the generic themes and feeding the number of keywords into\n",
    "a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "9126bf05-e873-471e-a7c5-44d7714cb7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Introduction</th>\n",
       "      <th>Methodology</th>\n",
       "      <th>Findings</th>\n",
       "      <th>Conclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Common Keywords</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unique to MySummary</th>\n",
       "      <td>68</td>\n",
       "      <td>29</td>\n",
       "      <td>82</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unique to LLM_Summary</th>\n",
       "      <td>56</td>\n",
       "      <td>42</td>\n",
       "      <td>48</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MySummary Common Ratio [%]</th>\n",
       "      <td>12%</td>\n",
       "      <td>17%</td>\n",
       "      <td>7%</td>\n",
       "      <td>12%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LLM_Summary Common Ratio [%]</th>\n",
       "      <td>15%</td>\n",
       "      <td>12%</td>\n",
       "      <td>12%</td>\n",
       "      <td>15%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Introduction Methodology Findings Conclusion\n",
       "Common Keywords                        10           6        7          3\n",
       "Unique to MySummary                    68          29       82         22\n",
       "Unique to LLM_Summary                  56          42       48         16\n",
       "MySummary Common Ratio [%]            12%         17%       7%        12%\n",
       "LLM_Summary Common Ratio [%]          15%         12%      12%        15%"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining themes that match the keys in my_sections and llm_sections dictionaries\n",
    "themes = ['Introduction', 'Methodology', 'Findings', 'Conclusion']\n",
    "\n",
    "# Prepare a dictionary to hold the counts for each theme\n",
    "data = {}\n",
    "\n",
    "for theme in themes:\n",
    "    # Get text for the theme; if a theme isn't found, default to an empty string.\n",
    "    my_text = my_sections.get(theme, \"\")\n",
    "    llm_text = llm_sections.get(theme, \"\")\n",
    "    \n",
    "    # Extract keywords from both texts\n",
    "    my_keywords = extract_keywords(my_text)\n",
    "    llm_keywords = extract_keywords(llm_text)\n",
    "    \n",
    "    # Calculate common and unique keyword sets\n",
    "    common_keywords = my_keywords.intersection(llm_keywords)\n",
    "    unique_to_my = my_keywords - llm_keywords\n",
    "    unique_to_llm = llm_keywords - my_keywords\n",
    "    \n",
    "    # Calculate total keywords for each summary\n",
    "    total_my = len(common_keywords) + len(unique_to_my)\n",
    "    total_llm = len(common_keywords) + len(unique_to_llm)\n",
    "    \n",
    "    # Calculate ratios and format as an integer followed by \"%\"\n",
    "    ratio_my = int((len(common_keywords) / total_my * 100)) if total_my > 0 else 0\n",
    "    ratio_llm = int((len(common_keywords) / total_llm * 100)) if total_llm > 0 else 0\n",
    "    \n",
    "    # Store the counts and formatted ratios in a sub-dictionary for this theme\n",
    "    data[theme] = {\n",
    "        'Common Keywords': len(common_keywords),\n",
    "        'Unique to MySummary': len(unique_to_my),\n",
    "        'Unique to LLM_Summary': len(unique_to_llm),\n",
    "        'MySummary Common Ratio [%]': f\"{ratio_my}%\",\n",
    "        'LLM_Summary Common Ratio [%]': f\"{ratio_llm}%\"\n",
    "    }\n",
    "\n",
    "# Create a DataFrame where columns are themes and rows are the categories\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optionally, reorder the rows\n",
    "desired_order = [\n",
    "    'Common Keywords', \n",
    "    'Unique to MySummary', \n",
    "    'Unique to LLM_Summary', \n",
    "    'MySummary Common Ratio [%]', \n",
    "    'LLM_Summary Common Ratio [%]'\n",
    "]\n",
    "df = df.reindex(desired_order)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebca0488-61d3-4a21-9dd2-de881e5f7283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f526cb-6c70-445c-9819-b439d4079329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4bbe39-b8ae-45d8-81d1-bcfe0c0074ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7040de79-1918-4bf5-96f3-7c973209c88c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40aab8d-a228-444c-b8dd-532174efd3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "849a832a-2aac-45b0-b12e-a45bc690a2e9",
   "metadata": {},
   "source": [
    "# Optional Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8058c7ae-c2e5-492b-9401-d778d84830e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0531825-e0ba-43b9-ba42-274cf7c6d894",
   "metadata": {},
   "source": [
    "## Reading the Original Study into a Markdown File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e191dfe5-8929-4825-b6a6-7357e6c967fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [..........................................................................] 3582534 / 3582534"
     ]
    }
   ],
   "source": [
    "# downloading the original study pdf\n",
    "import wget\n",
    "url = 'https://link.springer.com/content/pdf/10.1007/s10664-019-09693-x.pdf'\n",
    "pdf_filename = wget.download(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d153bab-6b8d-4f34-9fa5-a33c87d792ac",
   "metadata": {},
   "source": [
    "We are going to convert the original pdf file into markdown with Mistral AI.  \n",
    "We need to register and we can subscribe for a free plan on [Mistral](https://console.mistral.ai/home) homepage and we need to generate and API key.\n",
    "\n",
    "The API key can be stored in the current directory in a file called `.env` in format:\n",
    "```\n",
    "MISTRAL_API_KEY=Ta7...\n",
    "```\n",
    "We should add this file to the `.gitignore` in order to not upload in a public repository. This file can store more API keys in separate lines.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ed4112b3-6c3c-470a-a348-b200aeeaf8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Mistral client with API key\n",
    "from mistralai import Mistral\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# plain text API key can be used when notebook is not shared\n",
    "# api_key = \"your API key\" \n",
    "# In our case we are loading API KEY from .env file\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "client = Mistral(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "ad9fb4b5-a86e-4957-a073-603220ad68b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pathlib import Path\n",
    "from mistralai import DocumentURLChunk, ImageURLChunk, TextChunk\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "89e69533-9f7b-4a0f-a87b-277d86c6bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify PDF file exists\n",
    "pdf_file = Path(pdf_filename)\n",
    "assert pdf_file.is_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7134a84f-3916-4861-b226-b190344fad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload PDF file to Mistral's OCR service\n",
    "uploaded_file = client.files.upload(\n",
    "    file={\n",
    "        \"file_name\": pdf_file.stem,\n",
    "        \"content\": pdf_file.read_bytes(),\n",
    "    },\n",
    "    purpose=\"ocr\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "9225f3ac-482e-4761-a5d1-64d658fe0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get URL for the uploaded file\n",
    "signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=3)\n",
    "\n",
    "# Process PDF with OCR\n",
    "pdf_response = client.ocr.process(\n",
    "    document=DocumentURLChunk(document_url=signed_url.url),\n",
    "    model=\"mistral-ocr-latest\",\n",
    "    include_image_base64=True\n",
    ")\n",
    "\n",
    "# Convert response to JSON format\n",
    "response_dict = json.loads(pdf_response.model_dump_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af429c-9a73-4877-a887-9c8f7aababf8",
   "metadata": {},
   "source": [
    "There is a possibility to include the scanned tables and images as well (as demonstrated in the [colab file](https://colab.research.google.com/github/mistralai/cookbook/blob/main/mistral/ocr/structured_ocr.ipynb) shared by Mistral AI), but our experiment is going to use only the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "67980cf0-388a-4bef-93a9-52e8a2c0640e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pages', 'model', 'usage_info'])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the json object structure\n",
    "response_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "95e1fe7a-508d-417f-baa4-f1047748c523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the length of the scanned pdf\n",
    "len(response_dict['pages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba88bd9-fb92-4824-abab-1afc73c099ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "d3a4941a-b1c5-43ba-8b9d-5bb22e41acc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "lection as per the procedures discussed in Sections 3.3.1, 3.3.2 and 3.3.3. For each case study, the table provides the following information: (1) the number of omittable elements of different types: as explained in Section 3.3.2, an omittable element can be an entire requirement or a certain segment of a requirement, namely a condition, constraint, or object; (2) the number and type of interdependencies between the omittable constraints and objects (the types were discussed in Section 3.3.2); (3) interrater agreement, computed as Cohen's $\\kappa$ (1960), for the identification and classification of omittable segments by two coders. The $\\kappa$ scores indicate strong, almost perfect or perfect agreement in all case studies; (4) the number of domain model elements (of which the number of elements tacit in the requirements is shown in brackets); and (5) the number of trace links from the requirements to the domain model.\n",
       "\n",
       "## Algorithm Simulation\n",
       "\n",
       "Input: - A domain model\n",
       "\n",
       "- Set of requirements, their omittable segments, and the interdependencies between these segments\n",
       "- Set $\\mathcal{L}$ of trace links from the requirements to the domain model\n",
       "- Omission type $T \\in\\{\\mathrm{Req}$, Cond, Cons, Obj $\\}$ to simulate\n",
       "- Number $n$ of simulation runs\n",
       "\n",
       "Output: - Scatter plot Plot showing the percentage of unsupported domain model elements versus the number of omissions\n",
       "\n",
       "1. Let $\\mathcal{K}$ be the set of domain model elements not supported by $\\mathcal{L}$\n",
       "2. Let $\\mathcal{O}$ be the set of all omittable segments of type $T$\n",
       "3. Let Plot be initially empty\n",
       "4. for $i=1$ to $n$ :\n",
       "5. for $j=1$ to $|\\mathcal{O}|$ :\n",
       "6. Randomly pick $j$ elements, $\\mathcal{E}=\\left\\{e_{1}, \\cdots, e_{j}\\right\\}$, from $\\mathcal{O}$\n",
       "7. if $(T=$ Cons or $T=$ Obj $)$\n",
       "\n",
       "8 . Minimally modify $\\mathcal{E}$ to resolve any interdependency violations\n",
       "9. Let $\\mathcal{L}^{-}$be the set of all trace links originating from $\\mathcal{E}$\n",
       "10. $\\mathcal{L}^{\\prime}=\\mathcal{L} \\backslash \\mathcal{L}^{-}$\n",
       "11. Let $\\mathcal{U}$ be the set of domain model elements supported by $\\mathcal{L}$ but not $\\mathcal{L}^{\\prime}$\n",
       "12. Let $\\mathcal{X}$ be the domain elements in $\\mathcal{K}$ that will become unreachable from non- $\\mathcal{K}$ elements if $\\mathcal{U}$ is removed from the domain model\n",
       "13. Place the following datapoint on Plot: $(|\\mathcal{E}|,(|\\mathcal{U}|+|\\mathcal{X}|) / D)$, where $D$ is the total number of domain model elements\n",
       "14. return Plot\n",
       "\n",
       "Fig. 5 Simulation algorithm for computing sensitivity"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Displaying a random sample from the middle of the scanned pdf\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response_dict['pages'][17]['markdown'][555:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df77affa-ce39-4ae2-aed6-8475f0393ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "2af55892-f9ba-457e-a8d6-7bf50735be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the pages into a single Markdown file\n",
    "original_study = ''\n",
    "for page in response_dict['pages']:\n",
    "    original_study += page['markdown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "2213dbb6-0d69-42d1-9766-9929e8a06d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup of the original_study markdown file\n",
    "with open(\"original_study.md\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(original_study)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb766b-ebcc-4ef5-86b9-042c0c873f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a17f42f9-bcb2-4931-9f37-6fd334e454b0",
   "metadata": {},
   "source": [
    "## Summarization Quality Evaluation with ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e0d1e-7b38-4061-9757-41581ea5ad24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "35e11838-b2de-40c0-9cfa-949fe3246b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Summary vs Original Study:\n",
      "[{'rouge-1': {'r': 0.08398133748055987, 'p': 0.6506024096385542, 'f': 0.14876032855341545}, 'rouge-2': {'r': 0.01447051085288314, 'p': 0.19020172910662825, 'f': 0.02689486421162684}, 'rouge-l': {'r': 0.07900466562986003, 'p': 0.6120481927710844, 'f': 0.1399449015561703}}]\n",
      "LLM Summary vs Original Study:\n",
      "[{'rouge-1': {'r': 0.0656298600311042, 'p': 0.694078947368421, 'f': 0.11992043036238709}, 'rouge-2': {'r': 0.019403639552729664, 'p': 0.34368932038834954, 'f': 0.036733422252820475}, 'rouge-l': {'r': 0.06314152410575427, 'p': 0.6677631578947368, 'f': 0.1153736841276613}}]\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "# For the 'rouge' package, the order is candidate summary first, then reference.\n",
    "scores_my = rouge.get_scores(my_summary, original_study)\n",
    "scores_llm = rouge.get_scores(llm_summary, original_study)\n",
    "\n",
    "print(\"My Summary vs Original Study:\")\n",
    "print(scores_my)\n",
    "\n",
    "print(\"LLM Summary vs Original Study:\")\n",
    "print(scores_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "6b3590e1-4ba5-4177-a96f-e873cdfc237c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounded My Summary vs Original Study:\n",
      "[{'rouge-1': {'r': 0.08, 'p': 0.65, 'f': 0.15}, 'rouge-2': {'r': 0.01, 'p': 0.19, 'f': 0.03}, 'rouge-l': {'r': 0.08, 'p': 0.61, 'f': 0.14}}]\n",
      "Rounded LLM Summary vs Original Study:\n",
      "[{'rouge-1': {'r': 0.07, 'p': 0.69, 'f': 0.12}, 'rouge-2': {'r': 0.02, 'p': 0.34, 'f': 0.04}, 'rouge-l': {'r': 0.06, 'p': 0.67, 'f': 0.12}}]\n"
     ]
    }
   ],
   "source": [
    "def round_scores(score_list, decimals=2):\n",
    "    rounded_list = []\n",
    "    for score in score_list:\n",
    "        rounded_score = {}\n",
    "        for metric, values in score.items():\n",
    "            rounded_score[metric] = {k: round(v, decimals) for k, v in values.items()}\n",
    "        rounded_list.append(rounded_score)\n",
    "    return rounded_list\n",
    "\n",
    "# Assuming scores_my and scores_llm hold the results:\n",
    "rounded_my = round_scores(scores_my)\n",
    "rounded_llm = round_scores(scores_llm)\n",
    "\n",
    "print(\"Rounded My Summary vs Original Study:\")\n",
    "print(rounded_my)\n",
    "print(\"Rounded LLM Summary vs Original Study:\")\n",
    "print(rounded_llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dccd99-06d0-4595-9c5a-f56988729f0a",
   "metadata": {},
   "source": [
    "Feeding the results into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "9a107842-52c8-4965-874e-c239c9cec1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>My_Recall</th>\n",
       "      <th>My_Precision</th>\n",
       "      <th>My_F1</th>\n",
       "      <th>LLM_Recall</th>\n",
       "      <th>LLM_Precision</th>\n",
       "      <th>LLM_F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rouge-1</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rouge-2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rouge-l</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Metric  My_Recall  My_Precision  My_F1  LLM_Recall  LLM_Precision  LLM_F1\n",
       "0  rouge-1       0.08          0.65   0.15        0.07           0.69    0.12\n",
       "1  rouge-2       0.01          0.19   0.03        0.02           0.34    0.04\n",
       "2  rouge-l       0.08          0.61   0.14        0.06           0.67    0.12"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Build a DataFrame with rounded ROUGE scores for each metric and summary\n",
    "data = {\n",
    "    'Metric': ['rouge-1', 'rouge-2', 'rouge-l'],\n",
    "    'My_Recall': [round(scores_my[0]['rouge-1']['r'], 2), round(scores_my[0]['rouge-2']['r'], 2), round(scores_my[0]['rouge-l']['r'], 2)],\n",
    "    'My_Precision': [round(scores_my[0]['rouge-1']['p'], 2), round(scores_my[0]['rouge-2']['p'], 2), round(scores_my[0]['rouge-l']['p'], 2)],\n",
    "    'My_F1': [round(scores_my[0]['rouge-1']['f'], 2), round(scores_my[0]['rouge-2']['f'], 2), round(scores_my[0]['rouge-l']['f'], 2)],\n",
    "    'LLM_Recall': [round(scores_llm[0]['rouge-1']['r'], 2), round(scores_llm[0]['rouge-2']['r'], 2), round(scores_llm[0]['rouge-l']['r'], 2)],\n",
    "    'LLM_Precision': [round(scores_llm[0]['rouge-1']['p'], 2), round(scores_llm[0]['rouge-2']['p'], 2), round(scores_llm[0]['rouge-l']['p'], 2)],\n",
    "    'LLM_F1': [round(scores_llm[0]['rouge-1']['f'], 2), round(scores_llm[0]['rouge-2']['f'], 2), round(scores_llm[0]['rouge-l']['f'], 2)]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a365132-d310-4c04-9a35-98582046b1f0",
   "metadata": {},
   "source": [
    "Metrics Overview:\n",
    "\n",
    "- **ROUGE-1:** Measures overlap of individual words (unigrams) between the summary and the original study.\n",
    "- **ROUGE-2:** Checks the overlap of two-word sequences (bigrams), which gives a sense of phrase-level matching.\n",
    "- **ROUGE-L:** Looks at the longest common subsequence, capturing the overall structure or flow similarity.\n",
    "Recall, Precision, and F1:\n",
    "\n",
    "- **Recall (r):** Indicates what fraction of the original study’s content (words or sequences) is captured in the summary. Low recall (e.g., ~0.08 for My Summary and ~0.07 for LLM Summary in ROUGE-1) indicates that only a small portion of the original text appears in the summary.\n",
    "- **Precision (p):** Indicates what fraction of the summary’s content is relevant to the original study. Higher precision (around 0.65–0.70 for ROUGE-1) means that, although the summaries are short, most words they include are also found in the original.\n",
    "- **F1 Score (f):** The harmonic mean of recall and precision. A low F1 (around 0.12–0.15 for ROUGE-1) reflects the trade-off between the low recall and relatively high precision.\n",
    "  \n",
    "#### Comparing the Summaries:\n",
    "\n",
    "**My Summary:**  \n",
    "Slightly higher recall and F1 for ROUGE-1 and ROUGE-L suggest it captures a tad more of the original study's content structurally, even if only marginally.  \n",
    "**LLM Summary:**  \n",
    "Shows a little higher ROUGE-2 (bigram level) score, indicating marginally better phrase-level matches.\n",
    "\n",
    "Overall, both summaries are very concise relative to the original study. The low recall values signal that each summary includes only a small slice of the original text, while the high precision indicates that the included content is highly relevant. The differences between the two summaries are minor, with each having slight strengths in different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb7872-9aa3-4478-b3d9-849f33e8d297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7efafc94-3da5-49b7-b119-1922ee3aced6",
   "metadata": {},
   "source": [
    "## Summarization Quality Evaluation with BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "82d4ee4e-7466-4ec9-a7aa-d6682c0602e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "6db024a0-2a82-476e-9dce-20ddf1bd1cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the texts. \n",
    "# Advanced tokenization might be needed depending text\n",
    "reference_tokens = original_study.split()\n",
    "my_summary_tokens = my_summary.split()\n",
    "llm_summary_tokens = llm_summary.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "7b4314b7-799d-4ebc-ae1a-279cf00b59eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for My Summary vs Original Study: 1.2e-08\n",
      "BLEU score for LLM Summary vs Original Study: 4.2e-11\n"
     ]
    }
   ],
   "source": [
    "# Smoothing function to avoid zero scores for short texts\n",
    "smooth = SmoothingFunction().method7\n",
    "\n",
    "# Compute BLEU scores\n",
    "bleu_my = sentence_bleu([reference_tokens], my_summary_tokens, smoothing_function=smooth)\n",
    "bleu_llm = sentence_bleu([reference_tokens], llm_summary_tokens, smoothing_function=smooth)\n",
    "\n",
    "print(f\"BLEU score for My Summary vs Original Study: {bleu_my:.1e}\")\n",
    "print(f\"BLEU score for LLM Summary vs Original Study: {bleu_llm:.1e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185f2e6d-2518-46ed-850e-be640cd100f4",
   "metadata": {},
   "source": [
    "Metric overview:  \n",
    "\n",
    "- **BLEU Score:** Ranges from 0 to 1, where a higher score indicates more n-gram overlap between the candidate and reference.\n",
    "- **Smoothing:** Helps adjust for the brevity of summaries. Without it, shorter texts might unfairly get a score of 0.\n",
    "- **Usage:** Here BLEU provides a rough measure of similarity.\n",
    "Originally it was developed for translation tasks, so while useful,  its use in summarization (especially with abstractive summaries) often results in very low scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443f8309-c9f1-4f66-ad6e-0f9d25743bbf",
   "metadata": {},
   "source": [
    "#### Comparing the Summaries\n",
    "\n",
    "These extremely low BLEU scores (close to zero) indicate that the summaries share almost no direct n‑gram overlap with the original study. In other words, very few of the same word sequences appear in the summaries as in the original text.\n",
    "\n",
    "- **Length Discrepancy:** The summaries are much shorter than the original study, so the chance of overlapping n‑grams is inherently reduced. This is common in summarization tasks and can contribute to very low BLEU scores.\n",
    "\n",
    "- **Comparison Differences:** My Summary shows a slightly higher (though still extremely low) BLEU than LLM Summary.\n",
    "The fact that my_summary scores about 1.2e-08 compared to LLM Summary's 4.2e-11 implies that my_summary has marginally more overlapping phrases with the original study. This suggests that my_summary is slightly closer to the source wording than llm_summary.\n",
    "\n",
    "**Note**: BLEU is sensitive to literal matching. In the context of summarization, low BLEU scores are common and don't necessarily mean the summaries are poor; they just reflect the length difference and the degree of paraphrasing relative to the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449daa68-cd81-40b2-b3d5-e4a795c774a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e8eefc8-cc5d-4eae-8b22-d156e445f12d",
   "metadata": {},
   "source": [
    "## Summarization Quality Evaluation with BERT score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "eca452b3-ebb3-4b65-b477-190dd221eca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b4020b58dd41ed8e5daf756a993d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a423681f85e451cb76e7c03aad277e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.46 seconds, 0.58 sentences/sec\n",
      "BERTScore for My Summary vs Original Study:\n",
      "Precision: 0.83, Recall: 0.81, F1: 0.82\n",
      "\n",
      "BERTScore for LLM Summary vs Original Study:\n",
      "Precision: 0.85, Recall: 0.83, F1: 0.84\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "\n",
    "# Create lists of candidates and references. Here, we compare each summary to the original study.\n",
    "candidates = [my_summary, llm_summary]\n",
    "references = [original_study, original_study]\n",
    "\n",
    "# Compute BERTScore metrics. You can specify the language (e.g., \"en\") and set verbose=True to see progress.\n",
    "P, R, F1 = score(candidates, references, lang=\"en\", verbose=True)\n",
    "\n",
    "# Convert torch tensors to floats and round to 2 decimals for readability.\n",
    "my_precision, my_recall, my_f1 = round(P[0].item(), 2), round(R[0].item(), 2), round(F1[0].item(), 2)\n",
    "llm_precision, llm_recall, llm_f1 = round(P[1].item(), 2), round(R[1].item(), 2), round(F1[1].item(), 2)\n",
    "\n",
    "print(\"BERTScore for My Summary vs Original Study:\")\n",
    "print(f\"Precision: {my_precision}, Recall: {my_recall}, F1: {my_f1}\")\n",
    "\n",
    "print(\"\\nBERTScore for LLM Summary vs Original Study:\")\n",
    "print(f\"Precision: {llm_precision}, Recall: {llm_recall}, F1: {llm_f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265ade2-05b3-4429-8db9-feba699d1562",
   "metadata": {},
   "source": [
    "Metric overview  \n",
    "\n",
    "- **BERTScore:** BERTScore: Evaluates summaries by comparing semantic embeddings obtained from a pre-trained BERT model. Evaluates summaries by comparing semantic embeddings obtained from a pre-trained BERT model.\n",
    "- **Candidates & References:** Each summary (candidate) is compared to the original study (reference).\n",
    "- **Metrics:**  \n",
    "    - **Precision:** How much of the candidate is relevant to the reference.\n",
    "    - **Recall:** How much of the reference is captured by the candidate.\n",
    "    - **F1:** The harmonic mean of precision and recall, providing an overall quality measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b6c0e-f25b-42db-ba60-17505cdf98a3",
   "metadata": {},
   "source": [
    "#### Comparing the Summaries  \n",
    "\n",
    "Both summaries have high semantic similarity with the original study. The LLM summary has slightly higher scores across the board—precision (0.85 vs. 0.83), recall (0.83 vs. 0.81), and F1 (0.84 vs. 0.82). In plain terms, this suggests that while both summaries capture the essential meaning well, the LLM summary is marginally closer to the original text in terms of semantic content. However, the differences are quite small, indicating that both summaries are quite effective in representing the original study's ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d0762-35d5-4a71-93ac-918819d21630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
